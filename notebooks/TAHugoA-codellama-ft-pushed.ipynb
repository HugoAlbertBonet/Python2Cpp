{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10087360,"sourceType":"datasetVersion","datasetId":6219636},{"sourceId":10098879,"sourceType":"datasetVersion","datasetId":6228461}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-hf\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:37:35.568047Z","iopub.execute_input":"2024-12-03T10:37:35.571621Z","iopub.status.idle":"2024-12-03T10:40:00.467486Z","shell.execute_reply.started":"2024-12-03T10:37:35.571577Z","shell.execute_reply":"2024-12-03T10:40:00.466612Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e63c495bee2740068cf87382e2ddf532"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb216505275f47e9881dfc0671cc5140"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dce5560af2c74ef0a212f768f364b339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6344373de76f4d5aa0def099efc4d3b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84358c71f46a4eb9b7232e5a7dc85d3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3cd1aa3b3a34ac6bcaaf292d2fe933c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c42775c9e242e38ef94472cafff25c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"594a128c612e4c7095019bbcf46a9e22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a9b78d5907c4e92975b147f1a056742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90f49550913745c288145cd4652edf16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffacfdc94a364ccf89e8f64085c549a5"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:40:00.718879Z","iopub.execute_input":"2024-12-03T10:40:00.719114Z","iopub.status.idle":"2024-12-03T10:40:00.727443Z","shell.execute_reply.started":"2024-12-03T10:40:00.719088Z","shell.execute_reply":"2024-12-03T10:40:00.726492Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32016, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"tok = tokenizer.encode(\"def fibonacci():\", return_tensors = \"pt\")\nout = model(tok)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:40:00.728320Z","iopub.execute_input":"2024-12-03T10:40:00.728566Z","iopub.status.idle":"2024-12-03T10:40:03.342241Z","shell.execute_reply.started":"2024-12-03T10:40:00.728542Z","shell.execute_reply":"2024-12-03T10:40:03.341308Z"}},"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"codellama/CodeLlama-7b-hf\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nsequences = pipeline(\n    'def fibonacci():',\n    do_sample=True,\n    top_k=10,\n    temperature=0.01,\n    top_p=0.95,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:12:57.614623Z","iopub.execute_input":"2024-12-02T16:12:57.614975Z","iopub.status.idle":"2024-12-02T16:14:04.136284Z","shell.execute_reply.started":"2024-12-02T16:12:57.614946Z","shell.execute_reply":"2024-12-02T16:14:04.135384Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4719e67dc86b48c58744fea2d605f5cc"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Result: def fibonacci():\n    a, b = 0, 1\n    while True:\n        yield a\n        a, b = b, a + b\n\n\ndef fibonacci_sum_squares():\n    a, b = 0, 1\n    while True:\n        yield a * a + b * b\n        a, b = b, a + b\n\n\ndef fibonacci_sum():\n    a, b = 0, 1\n    while True:\n        yield a + b\n        a, b = b, a + b\n\n\ndef fibonacci_sum_squares_sum():\n    a, b = 0, 1\n    while True:\n        yield a * a + b * b + a + b\n        a, b = b, a + b\n\n\ndef fibonacci_sum_squares_sum\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"tokenizer(seq['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:14:04.162821Z","iopub.execute_input":"2024-12-02T16:14:04.163050Z","iopub.status.idle":"2024-12-02T16:14:04.169309Z","shell.execute_reply.started":"2024-12-02T16:14:04.163025Z","shell.execute_reply":"2024-12-02T16:14:04.168286Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [1, 822, 18755, 265, 21566, 7295, 13, 1678, 263, 29892, 289, 353, 29871, 29900, 29892, 29871, 29896, 13, 1678, 1550, 5852, 29901, 13, 4706, 7709, 263, 13, 4706, 263, 29892, 289, 353, 289, 29892, 263, 718, 289, 13, 13, 13, 1753, 18755, 265, 21566, 29918, 2083, 29918, 26613, 5114, 7295, 13, 1678, 263, 29892, 289, 353, 29871, 29900, 29892, 29871, 29896, 13, 1678, 1550, 5852, 29901, 13, 4706, 7709, 263, 334, 263, 718, 289, 334, 289, 13, 4706, 263, 29892, 289, 353, 289, 29892, 263, 718, 289, 13, 13, 13, 1753, 18755, 265, 21566, 29918, 2083, 7295, 13, 1678, 263, 29892, 289, 353, 29871, 29900, 29892, 29871, 29896, 13, 1678, 1550, 5852, 29901, 13, 4706, 7709, 263, 718, 289, 13, 4706, 263, 29892, 289, 353, 289, 29892, 263, 718, 289, 13, 13, 13, 1753, 18755, 265, 21566, 29918, 2083, 29918, 26613, 5114, 29918, 2083, 7295, 13, 1678, 263, 29892, 289, 353, 29871, 29900, 29892, 29871, 29896, 13, 1678, 1550, 5852, 29901, 13, 4706, 7709, 263, 334, 263, 718, 289, 334, 289, 718, 263, 718, 289, 13, 4706, 263, 29892, 289, 353, 289, 29892, 263, 718, 289, 13, 13, 13, 1753, 18755, 265, 21566, 29918, 2083, 29918, 26613, 5114, 29918, 2083], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"tok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:14:39.942589Z","iopub.execute_input":"2024-12-02T16:14:39.943440Z","iopub.status.idle":"2024-12-02T16:14:39.950560Z","shell.execute_reply.started":"2024-12-02T16:14:39.943392Z","shell.execute_reply":"2024-12-02T16:14:39.949500Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"tensor([[    1,   822, 18755,   265, 21566,  7295]])"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"out.logits.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:22:39.570674Z","iopub.execute_input":"2024-12-02T16:22:39.571572Z","iopub.status.idle":"2024-12-02T16:22:39.577878Z","shell.execute_reply.started":"2024-12-02T16:22:39.571526Z","shell.execute_reply":"2024-12-02T16:22:39.576877Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 6, 32016])"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"import torch\na = torch.randn(1, 3, 4)\nprint(a, a.size())\ntorch.argmax(out.logits, dim=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:23:42.003554Z","iopub.execute_input":"2024-12-02T16:23:42.003858Z","iopub.status.idle":"2024-12-02T16:23:42.012480Z","shell.execute_reply.started":"2024-12-02T16:23:42.003832Z","shell.execute_reply":"2024-12-02T16:23:42.011531Z"}},"outputs":[{"name":"stdout","text":"tensor([[[ 1.1353, -0.1615,  0.5686, -0.4768],\n         [ 0.4207, -0.6579, -0.5730,  0.5067],\n         [ 0.1075, -1.1442, -0.8909, -0.7239]]]) torch.Size([1, 3, 4])\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"tensor([[32007,  5453,   265, 21566, 29898,    13]])"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"torch.argmax(out.logits[:, -1]).item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:56:10.024557Z","iopub.execute_input":"2024-12-02T16:56:10.024884Z","iopub.status.idle":"2024-12-02T16:56:10.031164Z","shell.execute_reply.started":"2024-12-02T16:56:10.024854Z","shell.execute_reply":"2024-12-02T16:56:10.030395Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"13"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\nimport pandas as pd\nimport re\n\ndef retrieve_dataset(split:[\"train\", \"val\", \"test\"] = \"train\") -> Dataset:\n    \"\"\"\n    Retrieves a dataset of dictionaries with the codification:\n        {\"id\": id, \n        \"translation\":\n            {\"py\":pycode, \n            \"cpp\":cppcode}}\n    According to the split selected\n    \"\"\"\n\n    #Load the files\n    with open(f\"/kaggle/input/snippets/C++-Python/{split}-C++-map.jsonl\", \"r\") as f: cppids = f.read()\n    with open(f\"/kaggle/input/snippets/C++-Python/{split}-Python-map.jsonl\", \"r\") as f: pyids = f.read()\n    with open(f\"/kaggle/input/snippets/C++-Python/{split}-C++-Python-tok.cpp\", \"r\") as f: cppcode = f.read()\n    with open(f\"/kaggle/input/snippets/C++-Python/{split}-C++-Python-tok.py\", \"r\") as f: pycode = f.read()\n\n    #Divide the text\n    pyids = pyids.replace(\"Python\", \"py\"); pyids = re.findall(r\"(\\d+)-(py)-(\\d+)\", pyids)\n    cppids = cppids.replace(\"C++\", \"cpp\"); cppids = re.findall(r\"(\\d+)-(cpp)-(\\d+)\", cppids)\n    pycode = pycode.split(\"\\n\")[:-1]\n    cppcode = cppcode.split(\"\\n\")[:-1]\n\n    assert len(pycode) == len(pyids) and len(cppcode) == len(cppids) #Ids and lines of code are of equal length\n\n    ids = []\n    for i, lang, j in pyids:\n        if i not in ids:\n            ids.append(i)\n    \n    assert all(i in ids for i, lang, j in cppids) #Same ids for cpp and py\n\n    #Create list of dicts with the desired codification\n    idpy, idcpp = 0, 0\n    dataset = []\n    \n    for i in ids:\n        dic = {\"id\": i, \"translation\": {}}\n        pytrans, cpptrans = pycode[idpy], cppcode[idcpp]\n        idpy += 1; idcpp += 1\n        while idpy < len(pyids) and i in pyids[idpy]:\n            pytrans += \"\\n\" + pycode[idpy]\n            idpy += 1\n        while idcpp < len(cppids) and i in cppids[idcpp]:\n            cpptrans += \"\\n\" + cppcode[idcpp]\n            idcpp += 1\n    \n        dic[\"translation\"][\"py\"] = pytrans\n        dic[\"translation\"][\"cpp\"] = cpptrans\n        dataset.append(dic)\n\n\n    #Create the final dataset\n    split_ds = Dataset.from_pandas(pd.DataFrame(data=dataset))\n    return split_ds\n\n\ndef retrieve_all() -> DatasetDict:\n    \"\"\"\n    Retrieves a DatasetDict of Datasets cointaining the data of each split\n    \"\"\"\n    \n    train_ds = retrieve_dataset()\n    val_ds = retrieve_dataset(\"val\")\n    test_ds = retrieve_dataset(\"test\")\n    ds = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:50:55.270548Z","iopub.execute_input":"2024-12-05T10:50:55.271172Z","iopub.status.idle":"2024-12-05T10:50:55.280955Z","shell.execute_reply.started":"2024-12-05T10:50:55.271140Z","shell.execute_reply":"2024-12-05T10:50:55.280110Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"ds = retrieve_all()\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:51:00.036327Z","iopub.execute_input":"2024-12-05T10:51:00.037036Z","iopub.status.idle":"2024-12-05T10:51:17.130562Z","shell.execute_reply.started":"2024-12-05T10:51:00.037001Z","shell.execute_reply":"2024-12-05T10:51:17.129615Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 9308\n    })\n    validation: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 477\n    })\n    test: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 890\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"ds.save_to_disk('/kaggle/working/py2cpp')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:04:17.737916Z","iopub.execute_input":"2024-12-04T12:04:17.738326Z","iopub.status.idle":"2024-12-04T12:04:17.830571Z","shell.execute_reply.started":"2024-12-04T12:04:17.738291Z","shell.execute_reply":"2024-12-04T12:04:17.829647Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/9308 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f97966b00cde491e8f37c5c461a6f277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/477 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c05a347de0543a2b27ec43167d80a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/890 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35d1188422e047be9484cd5f502d6fc2"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"!zip -r /kaggle/working/py2cpp.zip /kaggle/working/py2cpp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:05:06.485226Z","iopub.execute_input":"2024-12-04T12:05:06.485614Z","iopub.status.idle":"2024-12-04T12:05:08.268474Z","shell.execute_reply.started":"2024-12-04T12:05:06.485582Z","shell.execute_reply":"2024-12-04T12:05:08.267458Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/py2cpp/ (stored 0%)\n  adding: kaggle/working/py2cpp/train/ (stored 0%)\n  adding: kaggle/working/py2cpp/train/dataset_info.json (deflated 61%)\n  adding: kaggle/working/py2cpp/train/state.json (deflated 38%)\n  adding: kaggle/working/py2cpp/train/data-00000-of-00001.arrow (deflated 80%)\n  adding: kaggle/working/py2cpp/test/ (stored 0%)\n  adding: kaggle/working/py2cpp/test/dataset_info.json (deflated 61%)\n  adding: kaggle/working/py2cpp/test/state.json (deflated 38%)\n  adding: kaggle/working/py2cpp/test/data-00000-of-00001.arrow (deflated 79%)\n  adding: kaggle/working/py2cpp/dataset_dict.json (deflated 5%)\n  adding: kaggle/working/py2cpp/validation/ (stored 0%)\n  adding: kaggle/working/py2cpp/validation/dataset_info.json (deflated 61%)\n  adding: kaggle/working/py2cpp/validation/state.json (deflated 38%)\n  adding: kaggle/working/py2cpp/validation/data-00000-of-00001.arrow (deflated 79%)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from datasets import load_from_disk\n\nds = load_from_disk('/kaggle/working/py2cpp')\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:46:20.713231Z","iopub.execute_input":"2024-12-05T10:46:20.713587Z","iopub.status.idle":"2024-12-05T10:46:20.741616Z","shell.execute_reply.started":"2024-12-05T10:46:20.713554Z","shell.execute_reply":"2024-12-05T10:46:20.740391Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_from_disk\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/py2cpp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m ds\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:2152\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2150\u001b[0m fs, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m url_to_fs(dataset_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2154\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2155\u001b[0m ):\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n","\u001b[0;31mFileNotFoundError\u001b[0m: Directory /kaggle/working/py2cpp not found"],"ename":"FileNotFoundError","evalue":"Directory /kaggle/working/py2cpp not found","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"# LLaMa tuto","metadata":{}},{"cell_type":"code","source":"!pip install datasets evaluate transformers==4.33.1 accelerate peft bitsandbytes --quiet\n!pip install sacrebleu --quiet\n!pip install huggingface_hub --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:40:33.518819Z","iopub.execute_input":"2024-12-09T00:40:33.519086Z","iopub.status.idle":"2024-12-09T00:41:12.568011Z","shell.execute_reply.started":"2024-12-09T00:40:33.519060Z","shell.execute_reply":"2024-12-09T00:41:12.566724Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from datasets import ClassLabel\nfrom datasets import Dataset, DatasetDict\nimport pandas as pd\nimport re\n\ndef retrieve_dataset(split:[\"train\", \"val\", \"test\"] = \"train\", dest_lang:[\"py\", \"cpp\", \"both\"] = \"cpp\") -> Dataset:\n    \"\"\"\n    Retrieves a dataset of dictionaries with the codification:\n        {\"id\": id, \n        \"translation\":\n            {\"py\":pycode, \n            \"cpp\":cppcode}}\n    According to the split selected\n    \"\"\"\n\n    #Load the files\n    with open(f\"/kaggle/input/snippets/C++-Python/{split}-C++-map.jsonl\", \"r\") as f: cppids = f.read()\n    with open(f\"/kaggle/input/snippets/C++-Python/{split}-Python-map.jsonl\", \"r\") as f: pyids = f.read()\n    with open(f\"/kaggle/input/snippets/C++-Python/{split}-C++-Python-tok.cpp\", \"r\") as f: cppcode = f.read()\n    with open(f\"/kaggle/input/snippets/C++-Python/{split}-C++-Python-tok.py\", \"r\") as f: pycode = f.read()\n\n    #Divide the text\n    pyids = pyids.replace(\"Python\", \"py\"); pyids = re.findall(r\"(\\d+)-(py)-(\\d+)\", pyids)\n    cppids = cppids.replace(\"C++\", \"cpp\"); cppids = re.findall(r\"(\\d+)-(cpp)-(\\d+)\", cppids)\n    pycode = pycode.split(\"\\n\")[:-1]\n    cppcode = cppcode.split(\"\\n\")[:-1]\n\n    assert len(pycode) == len(pyids) and len(cppcode) == len(cppids) #Ids and lines of code are of equal length\n\n    ids = []\n    for i, lang, j in pyids:\n        if i not in ids:\n            ids.append(i)\n    \n    assert all(i in ids for i, lang, j in cppids) #Same ids for cpp and py\n\n    #Create list of dicts with the desired codification\n    idpy, idcpp = 0, 0\n    dataset = []\n    \n    for i in ids:\n        dic = {\"source_text\": \"\", \"dest_text\": \"\", \"dest_lang\": \"\"}\n        pytrans, cpptrans = pycode[idpy], cppcode[idcpp]\n        idpy += 1; idcpp += 1\n        while idpy < len(pyids) and i in pyids[idpy]:\n            pytrans += \"\\n\" + pycode[idpy]\n            idpy += 1\n        while idcpp < len(cppids) and i in cppids[idcpp]:\n            cpptrans += \"\\n\" + cppcode[idcpp]\n            idcpp += 1\n\n        if dest_lang == \"cpp\" or dest_lang == \"both\":\n            dic[\"source_text\"]= pytrans\n            dic[\"dest_text\"] = cpptrans\n            dic[\"dest_lang\"] = \"cpp\"\n            dataset.append(dic)\n        if dest_lang == \"both\":\n            dic = {\"source_text\": \"\", \"dest_text\": \"\", \"dest_lang\": \"\"}\n        if dest_lang == \"py\" or dest_lang == \"both\":\n            dic[\"source_text\"]= cpptrans\n            dic[\"dest_text\"] = pytrans\n            dic[\"dest_lang\"] = \"py\"\n            dataset.append(dic)\n\n\n    #Create the final dataset\n    split_ds = Dataset.from_pandas(pd.DataFrame(data=dataset))\n    return split_ds\n\n\ndef retrieve_all() -> DatasetDict:\n    \"\"\"\n    Retrieves a DatasetDict of Datasets cointaining the data of each split\n    \"\"\"\n    \n    train_ds = retrieve_dataset()\n    val_ds = retrieve_dataset(\"val\")\n    test_ds = retrieve_dataset(\"test\")\n    ds = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n    return ds.class_encode_column(\"dest_lang\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:12.570292Z","iopub.execute_input":"2024-12-09T00:41:12.570681Z","iopub.status.idle":"2024-12-09T00:41:13.740700Z","shell.execute_reply.started":"2024-12-09T00:41:12.570641Z","shell.execute_reply":"2024-12-09T00:41:13.739825Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"ds = retrieve_all()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:13.744741Z","iopub.execute_input":"2024-12-09T00:41:13.745002Z","iopub.status.idle":"2024-12-09T00:41:30.322150Z","shell.execute_reply.started":"2024-12-09T00:41:13.744976Z","shell.execute_reply":"2024-12-09T00:41:30.321274Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Casting to class labels:   0%|          | 0/9308 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"662560329de8489fac548b2e7dde83fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting to class labels:   0%|          | 0/477 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9619e7950c7d4da9acda5bea7d12cc22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting to class labels:   0%|          | 0/890 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1bbddaa3974c1189e7afccdc68d5c7"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"ds[\"train\"].features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T16:05:16.254313Z","iopub.execute_input":"2024-12-05T16:05:16.254577Z","iopub.status.idle":"2024-12-05T16:05:16.260765Z","shell.execute_reply.started":"2024-12-05T16:05:16.254551Z","shell.execute_reply":"2024-12-05T16:05:16.259923Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'source_text': Value(dtype='string', id=None),\n 'dest_text': Value(dtype='string', id=None),\n 'dest_lang': ClassLabel(names=['cpp'], id=None)}"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:30.323184Z","iopub.execute_input":"2024-12-09T00:41:30.323453Z","iopub.status.idle":"2024-12-09T00:41:31.701539Z","shell.execute_reply.started":"2024-12-09T00:41:30.323426Z","shell.execute_reply":"2024-12-09T00:41:31.700333Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmax_tok_length = 128\n#checkpoint = \"meta-llama/Llama-2-7b-hf\"\ncheckpoint = \"codellama/CodeLlama-7b-hf\" \n#checkpoint = \"ajibawa-2023/Code-Llama-3-8B\"\ntokenizer = AutoTokenizer.from_pretrained(\n    checkpoint, use_auth_token=True,\n    padding=True,\n    pad_to_multiple_of=8,\n    truncation=True,\n    max_tok_len=max_tok_length,\n    padding_side='left',\n    )\ntokenizer.pad_token = \"[PAD]\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:31.703013Z","iopub.execute_input":"2024-12-09T00:41:31.703330Z","iopub.status.idle":"2024-12-09T00:41:34.475533Z","shell.execute_reply.started":"2024-12-09T00:41:31.703301Z","shell.execute_reply":"2024-12-09T00:41:34.474619Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1abf68caa6d4cbcb1c09e63e5e99146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ac761ca23bd4f99bc53070ccdc71700"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daa2ce9eb6cb4eb6a99b6b8f73bdb4fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be71700e79e94f28a7e2b4fcade13a1e"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def preprocess_function(sample):\n    model_inputs = tokenizer(\n        sample[\"source_text\"], \n        text_target = sample[\"dest_text\"],\n        )\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:34.476643Z","iopub.execute_input":"2024-12-09T00:41:34.477073Z","iopub.status.idle":"2024-12-09T00:41:34.481790Z","shell.execute_reply.started":"2024-12-09T00:41:34.477046Z","shell.execute_reply":"2024-12-09T00:41:34.480876Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tokenized_ds = ds.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:34.484397Z","iopub.execute_input":"2024-12-09T00:41:34.484801Z","iopub.status.idle":"2024-12-09T00:41:40.298283Z","shell.execute_reply.started":"2024-12-09T00:41:34.484765Z","shell.execute_reply":"2024-12-09T00:41:40.297385Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9308 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb214236d542479e80ea59b9f502f51e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/477 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b30157221cf42859fe71d8843c6c996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/890 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cbe655f1d6649948c9a0dfa99d9d825"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T17:11:08.682837Z","iopub.execute_input":"2024-12-06T17:11:08.683636Z","iopub.status.idle":"2024-12-06T17:11:08.689314Z","shell.execute_reply.started":"2024-12-06T17:11:08.683601Z","shell.execute_reply":"2024-12-06T17:11:08.688531Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['source_text', 'dest_text', 'dest_lang'],\n        num_rows: 9308\n    })\n    validation: Dataset({\n        features: ['source_text', 'dest_text', 'dest_lang'],\n        num_rows: 477\n    })\n    test: Dataset({\n        features: ['source_text', 'dest_text', 'dest_lang'],\n        num_rows: 890\n    })\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"tokenized_ds = tokenized_ds.filter(lambda x: len(x[\"input_ids\"]) <= max_tok_length and len(x[\"labels\"]) <= max_tok_length , desc=f\"Discarding source and target sentences with more than {max_tok_length} tokens\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:40.299342Z","iopub.execute_input":"2024-12-09T00:41:40.299612Z","iopub.status.idle":"2024-12-09T00:41:43.509169Z","shell.execute_reply.started":"2024-12-09T00:41:40.299586Z","shell.execute_reply":"2024-12-09T00:41:43.508308Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Discarding source and target sentences with more than 128 tokens:   0%|          | 0/9308 [00:00<?, ? examples…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce919eca750e4d83bbf511b5700bbd09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Discarding source and target sentences with more than 128 tokens:   0%|          | 0/477 [00:00<?, ? examples/…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"504e2c639fe543c6a4885bd32ffae6b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Discarding source and target sentences with more than 128 tokens:   0%|          | 0/890 [00:00<?, ? examples/…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39dba4db06dc4fe0befeccea3996588c"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenized_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:51:22.549625Z","iopub.execute_input":"2024-12-07T11:51:22.549875Z","iopub.status.idle":"2024-12-07T11:51:22.555739Z","shell.execute_reply.started":"2024-12-07T11:51:22.549851Z","shell.execute_reply":"2024-12-07T11:51:22.554889Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['source_text', 'dest_text', 'dest_lang', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 1079\n    })\n    validation: Dataset({\n        features: ['source_text', 'dest_text', 'dest_lang', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 56\n    })\n    test: Dataset({\n        features: ['source_text', 'dest_text', 'dest_lang', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 133\n    })\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"dic = []\nfor sample in tokenized_ds['train']:\n    sample_length = len(sample['input_ids'])\n    dic.append(sample_length)\n\nimport pandas as pd\ndf = pd.DataFrame({\"length\":dic})\ndf.hist(bins = 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:43.510216Z","iopub.execute_input":"2024-12-09T00:41:43.510486Z","iopub.status.idle":"2024-12-09T00:41:44.100872Z","shell.execute_reply.started":"2024-12-09T00:41:43.510459Z","shell.execute_reply":"2024-12-09T00:41:44.100013Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([[<Axes: title={'center': 'length'}>]], dtype=object)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import torch\n\nsrc = \"py\"\ntgt = \"cpp\"\ntask_prefix = f\"Translate from {src} to {tgt}:\\n\"\ns = \"\"\n\n\nif \"Llama-3\" in checkpoint: tokenizer.pad_token_id = 128002 \n\nprefix_tok_len = len(tokenizer.encode(f\"{task_prefix}{src}: {s} = {tgt}: \"))\nmax_tok_len = prefix_tok_len\n# Adding 2 for new line in target sentence and eos_token_id token\nmax_tok_len += 2 * max_tok_length + 2\n\n\ndef preprocess4training_function(sample):\n    \n    sample_size = len(sample[\"source_text\"])\n\n    # Creating the prompt with the task description for each source sentence\n    inputs  = [f\"{task_prefix}{src}: {s} = {tgt}: \" for s in sample[\"source_text\"]]\n\n    # Appending new line after each sample in the batch\n    targets = [f\"{s}\\n\" for s in sample[\"dest_text\"]]\n\n    # Applying the Llama2 tokenizer to the inputs and targets \n    # to obtain \"input_ids\" (token_ids) and \"attention mask\" \n    model_inputs = tokenizer(inputs)\n    labels = tokenizer(targets)\n    \n    # Each input is appended with its target \n    # Each target is prepended with as many special token id (-100) as the original input length\n    # Both input and target (label) has the same max_tok_len\n    # Attention mask is all 1s \n    for i in range(sample_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n\n    # Each input is applied left padding up to max_tok_len\n    # Attention mask is 0 for padding\n    # Each target (label) is left filled with special token id (-100)\n    # Finally inputs, attention_mask and targets (labels) are truncated to max_tok_len\n    for i in range(sample_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i]\n        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n            max_tok_len - len(sample_input_ids)\n        ) + sample_input_ids\n        model_inputs[\"attention_mask\"][i] = [0] * (max_tok_len - len(sample_input_ids)) + model_inputs[\n            \"attention_mask\"\n        ][i]\n        labels[\"input_ids\"][i] = [-100] * (max_tok_len - len(sample_input_ids)) + label_input_ids\n        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_tok_len])\n        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_tok_len])\n        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_tok_len])\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:44.102062Z","iopub.execute_input":"2024-12-09T00:41:44.102333Z","iopub.status.idle":"2024-12-09T00:41:47.006369Z","shell.execute_reply.started":"2024-12-09T00:41:44.102306Z","shell.execute_reply":"2024-12-09T00:41:47.005691Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def preprocess4test_function(sample):\n    inputs = [f\"{task_prefix}{src}: {s} = {tgt}: \" for s in sample[\"source_text\"]]\n    model_inputs = tokenizer(inputs,padding=True,)\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:47.007391Z","iopub.execute_input":"2024-12-09T00:41:47.007720Z","iopub.status.idle":"2024-12-09T00:41:47.012567Z","shell.execute_reply.started":"2024-12-09T00:41:47.007684Z","shell.execute_reply":"2024-12-09T00:41:47.011727Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"preprocessed_train_dataset = tokenized_ds['train'].map(preprocess4training_function, batched=True)\npreprocessed_dev_dataset = tokenized_ds['validation'].map(preprocess4training_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:47.015549Z","iopub.execute_input":"2024-12-09T00:41:47.016097Z","iopub.status.idle":"2024-12-09T00:41:47.721760Z","shell.execute_reply.started":"2024-12-09T00:41:47.016070Z","shell.execute_reply":"2024-12-09T00:41:47.720879Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1079 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6abe1ae399146f988ff55a66f03cdbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/56 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b28ab1126404e379ba87429d0d83d6b"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"preprocessed_test_dataset = tokenized_ds['test'].map(preprocess4test_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:47.722720Z","iopub.execute_input":"2024-12-09T00:41:47.722987Z","iopub.status.idle":"2024-12-09T00:41:47.790258Z","shell.execute_reply.started":"2024-12-09T00:41:47.722961Z","shell.execute_reply":"2024-12-09T00:41:47.789410Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/133 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0518223be174b0f8627e3978d212c09"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:47.791192Z","iopub.execute_input":"2024-12-09T00:41:47.791418Z","iopub.status.idle":"2024-12-09T00:41:47.798181Z","shell.execute_reply.started":"2024-12-09T00:41:47.791395Z","shell.execute_reply":"2024-12-09T00:41:47.797334Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint,\n    quantization_config=quantization_config,\n    torch_dtype=torch.bfloat16,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:41:47.799084Z","iopub.execute_input":"2024-12-09T00:41:47.799326Z","iopub.status.idle":"2024-12-09T00:43:15.121435Z","shell.execute_reply.started":"2024-12-09T00:41:47.799302Z","shell.execute_reply":"2024-12-09T00:43:15.120690Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e01e4dfcfb4f9ab4c83d0d91f2768d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c1c3482dedb49ccb09d3556f68cc77b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c15a451c5248adbc7e2a97bf8f1a80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8238c4cd42094a59905031ef495a6843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc2e1b3c72fd46b5ba05291be75fa56a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba25cfc3c60342f198b427c3919a2c7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d98949249f1247728ca3bc8a8daebe72"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"!pip install peft==0.13 --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:43:15.122549Z","iopub.execute_input":"2024-12-09T00:43:15.123203Z","iopub.status.idle":"2024-12-09T00:43:23.951459Z","shell.execute_reply.started":"2024-12-09T00:43:15.123160Z","shell.execute_reply":"2024-12-09T00:43:23.950582Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False, gradient_checkpointing_kwargs={'use_reentrant':False})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:43:23.955681Z","iopub.execute_input":"2024-12-09T00:43:23.956309Z","iopub.status.idle":"2024-12-09T00:43:24.047643Z","shell.execute_reply.started":"2024-12-09T00:43:23.956279Z","shell.execute_reply":"2024-12-09T00:43:24.047067Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    inference_mode=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:43:24.048482Z","iopub.execute_input":"2024-12-09T00:43:24.048729Z","iopub.status.idle":"2024-12-09T00:43:24.052967Z","shell.execute_reply.started":"2024-12-09T00:43:24.048704Z","shell.execute_reply":"2024-12-09T00:43:24.052062Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"lora_model = get_peft_model(model, config)\nlora_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:43:24.054012Z","iopub.execute_input":"2024-12-09T00:43:24.054256Z","iopub.status.idle":"2024-12-09T00:43:24.258879Z","shell.execute_reply.started":"2024-12-09T00:43:24.054223Z","shell.execute_reply":"2024-12-09T00:43:24.258004Z"}},"outputs":[{"name":"stdout","text":"trainable params: 8,388,608 || all params: 6,746,935,296 || trainable%: 0.1243\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:43:24.260079Z","iopub.execute_input":"2024-12-09T00:43:24.260442Z","iopub.status.idle":"2024-12-09T00:43:24.879283Z","shell.execute_reply.started":"2024-12-09T00:43:24.260404Z","shell.execute_reply":"2024-12-09T00:43:24.878597Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nbatch_size = 1\ngradient_accumulation_steps = 16\nmodel_name = checkpoint.split(\"/\")[-1]\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-py-to-cpp\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=3,\n    warmup_steps=100,\n    optim=\"adamw_bnb_8bit\",\n    prediction_loss_only=True,\n    gradient_accumulation_steps = gradient_accumulation_steps,\n    fp16=True,\n    group_by_length=True,\n    push_to_hub=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:43:24.880204Z","iopub.execute_input":"2024-12-09T00:43:24.881216Z","iopub.status.idle":"2024-12-09T00:43:24.917297Z","shell.execute_reply.started":"2024-12-09T00:43:24.881179Z","shell.execute_reply":"2024-12-09T00:43:24.916487Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    lora_model,\n    args,\n    train_dataset=preprocessed_train_dataset,\n    eval_dataset=preprocessed_dev_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:43:24.918366Z","iopub.execute_input":"2024-12-09T00:43:24.919112Z","iopub.status.idle":"2024-12-09T00:43:25.792577Z","shell.execute_reply.started":"2024-12-09T00:43:24.919075Z","shell.execute_reply":"2024-12-09T00:43:25.791534Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:43:25.793947Z","iopub.execute_input":"2024-12-09T00:43:25.794621Z","iopub.status.idle":"2024-12-09T04:32:45.844193Z","shell.execute_reply.started":"2024-12-09T00:43:25.794579Z","shell.execute_reply":"2024-12-09T04:32:45.843329Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113466977778645, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a040a69b3f144b85b28f79f9e6bc5066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241209_004350-r2ajn515</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hyperloopupv/huggingface/runs/r2ajn515' target=\"_blank\">fallen-morning-28</a></strong> to <a href='https://wandb.ai/hyperloopupv/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hyperloopupv/huggingface' target=\"_blank\">https://wandb.ai/hyperloopupv/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hyperloopupv/huggingface/runs/r2ajn515' target=\"_blank\">https://wandb.ai/hyperloopupv/huggingface/runs/r2ajn515</a>"},"metadata":{}},{"name":"stderr","text":"You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='99' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [99/99 3:46:59, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>1.330375</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.802413</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.498230</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=99, training_loss=1.0718118686868687, metrics={'train_runtime': 13758.9682, 'train_samples_per_second': 0.235, 'train_steps_per_second': 0.007, 'total_flos': 1.796608665255936e+16, 'train_loss': 1.0718118686868687, 'epoch': 2.93})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"trainer.push_to_hub(commit_message=\"Training complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:32:45.845442Z","iopub.execute_input":"2024-12-09T04:32:45.845807Z","iopub.status.idle":"2024-12-09T04:32:49.845137Z","shell.execute_reply.started":"2024-12-09T04:32:45.845769Z","shell.execute_reply":"2024-12-09T04:32:49.844228Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"527dd2ca3bdd4044944722087e395caf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/33.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a782c76b02f9464492cb72b118d1fc6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40f50309a34c4b86aea8ecc2dbcc0a87"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/hugo-albert/CodeLlama-7b-hf-finetuned-py-to-cpp/commit/ac9492631573252eff8aab31cfa490c73a650f14', commit_message='Training complete', commit_description='', oid='ac9492631573252eff8aab31cfa490c73a650f14', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hugo-albert/CodeLlama-7b-hf-finetuned-py-to-cpp', endpoint='https://huggingface.co', repo_type='model', repo_id='hugo-albert/CodeLlama-7b-hf-finetuned-py-to-cpp'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from transformers import GenerationConfig\n\ngeneration_config = GenerationConfig.from_pretrained(\n    checkpoint,\n)\n\nprint(generation_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:32:49.846230Z","iopub.execute_input":"2024-12-09T04:32:49.846503Z","iopub.status.idle":"2024-12-09T04:32:49.947205Z","shell.execute_reply.started":"2024-12-09T04:32:49.846474Z","shell.execute_reply":"2024-12-09T04:32:49.946541Z"}},"outputs":[{"name":"stdout","text":"GenerationConfig {\n  \"_from_model_config\": true,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"transformers_version\": \"4.33.1\"\n}\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"test_batch_size = 4\nbatch_tokenized_test = preprocessed_test_dataset.batch(test_batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:32:49.948211Z","iopub.execute_input":"2024-12-09T04:32:49.948548Z","iopub.status.idle":"2024-12-09T04:32:50.050083Z","shell.execute_reply.started":"2024-12-09T04:32:49.948510Z","shell.execute_reply":"2024-12-09T04:32:50.049256Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batching examples:   0%|          | 0/133 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c56cfc33fd4546208df944e536044d50"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"number_of_batches = len(batch_tokenized_test[\"input_ids\"])\noutput_sequences = []\nfor i in range(number_of_batches):\n    output_batch = lora_model.generate(\n        generation_config=generation_config, \n        input_ids=torch.tensor(batch_tokenized_test[\"input_ids\"][i]).cuda(), \n        attention_mask=torch.tensor(batch_tokenized_test[\"attention_mask\"][i]).cuda(), \n        max_length = max_tok_len, \n        num_beams=1, \n        do_sample=False,)\n    output_sequences.extend(output_batch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:32:50.051130Z","iopub.execute_input":"2024-12-09T04:32:50.051373Z","iopub.status.idle":"2024-12-09T05:16:17.301192Z","shell.execute_reply.started":"2024-12-09T04:32:50.051349Z","shell.execute_reply":"2024-12-09T05:16:17.300458Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"!pip install unbabel-comet --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T05:16:17.304016Z","iopub.execute_input":"2024-12-09T05:16:17.304276Z","iopub.status.idle":"2024-12-09T05:16:28.627570Z","shell.execute_reply.started":"2024-12-09T05:16:17.304251Z","shell.execute_reply":"2024-12-09T05:16:28.626738Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.5 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from evaluate import load\n\ncomet = load(\"comet\")\nbleu = load(\"sacrebleu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T05:16:28.628955Z","iopub.execute_input":"2024-12-09T05:16:28.629222Z","iopub.status.idle":"2024-12-09T05:17:00.743712Z","shell.execute_reply.started":"2024-12-09T05:16:28.629196Z","shell.execute_reply":"2024-12-09T05:17:00.742897Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8831f221990c4da595b179d933aa18b6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dd7ab96061d4dc9adecc8ffcf02a371"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE:   0%|          | 0.00/9.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee0d0250ff6d447c994ce94cb187699e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e77e9de3cfd44f2ba5e4f914dc3532b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0541c524be314bc0aaad4a4dd618599f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hparams.yaml:   0%|          | 0.00/567 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec160a31cdb48eeabaebc34fbd45b81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.ckpt:   0%|          | 0.00/2.32G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc0fb37feb614c0eb8e4d550a9f56301"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746eb9a7501244fa9a5e5f5bb740f177"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71225d1d73a1440ab7b43b9815945e20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"556329b65ab94f1e88d6b712317980c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9f695877e64f42ae89b28e0f5a1be4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed2fbeb9687400a9a3d23d5b8589978"}},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import re\n\ndef compute_metrics(sample, output_sequences):\n    inputs = [f\"{task_prefix}{src}: {s} = {tgt}: \"  for s in sample[\"source_text\"]]\n    preds = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n    #print(inputs)\n    #print(preds)\n    for i, (input,pred) in enumerate(zip(inputs,preds)):\n      pred = re.search(r'^.*\\n',pred.removeprefix(input).lstrip())\n      if pred is not None:\n        preds[i] = pred.group()[:-1]\n      else:\n        preds[i] = \"\"\n    #print(sample[\"source_text\"])\n    #print(sample[\"dest_text\"])\n    #print(preds)\n    resultcomet = comet.compute(sources = sample[\"source_text\"], predictions=preds, references=sample[\"dest_text\"])\n    resultbleu = bleu.compute(predictions=preds, references=sample[\"dest_text\"])\n    result = {\"bleu\": resultbleu[\"score\"], \"comet\": resultcomet[\"mean_score\"], \"comet_all\": resultcomet[\"scores\"]}\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T05:17:00.744722Z","iopub.execute_input":"2024-12-09T05:17:00.745385Z","iopub.status.idle":"2024-12-09T05:17:00.752165Z","shell.execute_reply.started":"2024-12-09T05:17:00.745354Z","shell.execute_reply":"2024-12-09T05:17:00.751328Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"result = compute_metrics(preprocessed_test_dataset,output_sequences)\nprint(f'BLEU score: {result[\"bleu\"]}')\nprint(f'COMET score: {result[\"comet\"]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T05:17:00.753050Z","iopub.execute_input":"2024-12-09T05:17:00.753273Z","iopub.status.idle":"2024-12-09T05:17:13.704897Z","shell.execute_reply.started":"2024-12-09T05:17:00.753250Z","shell.execute_reply":"2024-12-09T05:17:13.703987Z"}},"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"BLEU score: 0.02769391976233836\nCOMET score: 0.27834762644050715\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(result[\"comet_all\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T05:17:13.706155Z","iopub.execute_input":"2024-12-09T05:17:13.706430Z","iopub.status.idle":"2024-12-09T05:17:13.949014Z","shell.execute_reply.started":"2024-12-09T05:17:13.706402Z","shell.execute_reply":"2024-12-09T05:17:13.948221Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"(array([59., 62.,  5.,  0.,  0.,  2.,  0.,  1.,  2.,  2.]),\n array([0.18570663, 0.25158174, 0.31745684, 0.38333195, 0.44920705,\n        0.51508216, 0.58095726, 0.64683237, 0.71270747, 0.77858258,\n        0.84445769]),\n <BarContainer object of 10 artists>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"def check_translation(i):\n    print(f\"COMET of snippet {i}:\", result[\"comet_all\"][i])\n    print(\"REAL: \\n\", preprocessed_test_dataset[i][\"dest_text\"].replace(\"NEW_LINE\", \"\\n\"))\n    print(\"PRED: \\n\", tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[i].replace(\"NEW_LINE\", \"\\n\").split(\"= cpp: \")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T05:17:13.949950Z","iopub.execute_input":"2024-12-09T05:17:13.950247Z","iopub.status.idle":"2024-12-09T05:17:13.955828Z","shell.execute_reply.started":"2024-12-09T05:17:13.950218Z","shell.execute_reply":"2024-12-09T05:17:13.954998Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"for i in range(len(result[\"comet_all\"])):\n    if result[\"comet_all\"][i] > 0.75:\n        check_translation(i)\n        break\n\nprint(\"===================\")\nfor i in range(len(result[\"comet_all\"])):\n    if result[\"comet_all\"][i] < 0.3:\n        check_translation(i)\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T05:17:13.956905Z","iopub.execute_input":"2024-12-09T05:17:13.957156Z","iopub.status.idle":"2024-12-09T05:17:14.028878Z","shell.execute_reply.started":"2024-12-09T05:17:13.957132Z","shell.execute_reply":"2024-12-09T05:17:14.028054Z"}},"outputs":[{"name":"stdout","text":"COMET of snippet 41: 0.8444576859474182\nREAL: \n int maxvolume ( int s ) {\nint length = s / 3 ; s -= length ;\nint breadth = s / 2 ;\nint height = s - breadth ; return length * breadth * height ; }\nint main ( ) { int s = 8 ; cout << maxvolume ( s ) << endl ; return 0 ; }\nPRED: \n  #include <bits/stdc++.h> \n using namespace std ; int maxvolume ( int s ) { int length = s / 3 ; s -= length ; int breadth = s / 2 ; int height = s - breadth ; return length * breadth * height ; } int main ( ) { int s = 8 ; cout << maxvolume ( s ) ; return 0 ; }\n\n===================\nCOMET of snippet 0: 0.22882574796676636\nREAL: \n void checkSolution ( int a , int b , int c ) { if ( a == c ) cout << \" Yes \" ; else cout << \" No \" ; }\nint main ( ) { int a = 2 , b = 0 , c = 2 ; checkSolution ( a , b , c ) ; return 0 ; }\nPRED: \n  Tags: pyopencv py cpp\n\npy: def checkSolution ( a , b , c ) : \n INDENT if ( a == c ) : \n INDENT print ( \" Yes \" ) ; \n DEDENT else : \n INDENT print ( \" No \" ) ; \n DEDENT DEDENT\na = 2 ; b = 0 ; c = 2 ; \n checkSolution ( a , b , c ) ; \n \n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# HF tuto","metadata":{}},{"cell_type":"code","source":"!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:51:30.760175Z","iopub.execute_input":"2024-12-05T10:51:30.761066Z","iopub.status.idle":"2024-12-05T10:51:32.149104Z","shell.execute_reply.started":"2024-12-05T10:51:30.761030Z","shell.execute_reply":"2024-12-05T10:51:32.148124Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = \"google-t5/t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:51:33.124162Z","iopub.execute_input":"2024-12-05T10:51:33.125120Z","iopub.status.idle":"2024-12-05T10:51:39.025542Z","shell.execute_reply.started":"2024-12-05T10:51:33.125083Z","shell.execute_reply":"2024-12-05T10:51:39.024830Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f06a875e67d4f2c808e0d9ed42d0c90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a65809c92e4b457f88a47540ca9a87c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fbf102220b6423ab1785a8b6f8cc75e"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"source_lang = \"py\"\ntarget_lang = \"cpp\"\nprefix = \"translate Python to C++: \"\n\n\ndef preprocess_function(examples):\n    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n    targets = [example[target_lang] for example in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:51:39.026968Z","iopub.execute_input":"2024-12-05T10:51:39.027410Z","iopub.status.idle":"2024-12-05T10:51:39.032348Z","shell.execute_reply.started":"2024-12-05T10:51:39.027382Z","shell.execute_reply":"2024-12-05T10:51:39.031482Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tokenized_ds = ds.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:51:39.033459Z","iopub.execute_input":"2024-12-05T10:51:39.033814Z","iopub.status.idle":"2024-12-05T10:51:46.627047Z","shell.execute_reply.started":"2024-12-05T10:51:39.033782Z","shell.execute_reply":"2024-12-05T10:51:46.626034Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9308 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a9de2602604ed1ac9a135faf5799ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/477 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ccb9f7872944ff92d610fe501c5c4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/890 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bce4728adf784c36826ab92f7cb27352"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:51:46.629289Z","iopub.execute_input":"2024-12-05T10:51:46.629893Z","iopub.status.idle":"2024-12-05T10:51:59.937920Z","shell.execute_reply.started":"2024-12-05T10:51:46.629851Z","shell.execute_reply":"2024-12-05T10:51:59.937200Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!pip install evaluate sacrebleu --quiet\nimport evaluate\n\nmetric = evaluate.load(\"sacrebleu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:51:59.939102Z","iopub.execute_input":"2024-12-05T10:51:59.939908Z","iopub.status.idle":"2024-12-05T10:52:12.785490Z","shell.execute_reply.started":"2024-12-05T10:51:59.939865Z","shell.execute_reply":"2024-12-05T10:52:12.784503Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca93c5dda5f549c3925267875ca14167"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\n\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n\n    return preds, labels\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:52:12.787004Z","iopub.execute_input":"2024-12-05T10:52:12.787555Z","iopub.status.idle":"2024-12-05T10:52:12.794696Z","shell.execute_reply.started":"2024-12-05T10:52:12.787526Z","shell.execute_reply":"2024-12-05T10:52:12.793710Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!pip install -U transformers --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:56:55.333654Z","iopub.execute_input":"2024-12-05T10:56:55.334049Z","iopub.status.idle":"2024-12-05T10:57:03.922877Z","shell.execute_reply.started":"2024-12-05T10:56:55.334021Z","shell.execute_reply":"2024-12-05T10:57:03.921814Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:57:50.388479Z","iopub.execute_input":"2024-12-05T10:57:50.389490Z","iopub.status.idle":"2024-12-05T10:57:50.779886Z","shell.execute_reply.started":"2024-12-05T10:57:50.389439Z","shell.execute_reply":"2024-12-05T10:57:50.779108Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"hf_tuto\",\n    eval_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=10,\n    predict_with_generate=True,\n    fp16=True, #change to bf16=True for XPU\n    push_to_hub=True,\n)\n\ntraining_args.average_tokens_across_devices = False\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds[\"train\"],\n    eval_dataset=tokenized_ds[\"validation\"],\n    tokenizer=tokenizer, #change to processing_class = tokenizer if higher version of transformers\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:00:17.757455Z","iopub.execute_input":"2024-12-05T11:00:17.757828Z","iopub.status.idle":"2024-12-05T11:02:06.088066Z","shell.execute_reply.started":"2024-12-05T11:00:17.757796Z","shell.execute_reply":"2024-12-05T11:02:06.086611Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2036904598.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='292' max='2910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 292/2910 01:45 < 15:52, 2.75 it/s, Epoch 1/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  # 2. reshape scores as [batch_size*vocab_size, # generation steps] with # generation steps being\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 27\u001b[0m\n\u001b[1;32m     15\u001b[0m training_args\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2114\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2112\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2113\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2115\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2121\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2573\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2573\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2577\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3004\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3002\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3004\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2958\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2958\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2959\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2961\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:195\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3975\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3972\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3974\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3975\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3985\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:4169\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4166\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   4168\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 4169\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4170\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4171\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4173\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:331\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m summon_full_params_context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    325\u001b[0m     FullyShardedDataParallel\u001b[38;5;241m.\u001b[39msummon_full_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, FullyShardedDataParallel)\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m    328\u001b[0m )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m summon_full_params_context:\n\u001b[0;32m--> 331\u001b[0m     generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2048\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3001\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2991\u001b[0m     next_model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(\n\u001b[1;32m   2992\u001b[0m         top_k_ids[:, selected_idx]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[1;32m   2993\u001b[0m     )\n\u001b[1;32m   2995\u001b[0m     selected_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[1;32m   2996\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnext_model_input,\n\u001b[1;32m   2997\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2998\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2999\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   3000\u001b[0m     )\n\u001b[0;32m-> 3001\u001b[0m     next_past_key_values \u001b[38;5;241m=\u001b[39m selected_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3003\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3004\u001b[0m     _, next_past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_past_from_model_output(outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:346\u001b[0m, in \u001b[0;36mprepare_inputs_for_generation\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGenerationMixin\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    A class containing all functions for auto-regressive text generation, to be used as a mixin in [`PreTrainedModel`].\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    The class exposes [`~generation.GenerationMixin.generate`], which can be used for:\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m        - *greedy decoding* if `num_beams=1` and `do_sample=False`\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m        - *contrastive search* if `penalty_alpha>0` and `top_k>1`\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m        - *multinomial sampling* if `num_beams=1` and `do_sample=True`\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m        - *beam-search decoding* if `num_beams>1` and `do_sample=False`\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m        - *beam-search multinomial sampling* if `num_beams>1` and `do_sample=True`\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m        - *diverse beam-search decoding* if `num_beams>1` and `num_beam_groups>1`\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m        - *constrained beam-search decoding* if `constraints!=None` or `force_words_ids!=None`\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        - *assisted decoding* if `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    To learn more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_inputs_for_generation\u001b[39m(\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m         input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    356\u001b[0m     ):\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m        Prepare the model inputs for generation. In includes operations like computing the 4D attention mask or\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        slicing inputs given the existing cache.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m        requirements for e.g. `past_key_values`). This function should work as is for most LLMs.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n","\u001b[0;31mNotImplementedError\u001b[0m: A model class needs to define a `prepare_inputs_for_generation` method in order to use `.generate()`."],"ename":"NotImplementedError","evalue":"A model class needs to define a `prepare_inputs_for_generation` method in order to use `.generate()`.","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\ntext = tokenized\ntranslator = pipeline(\"translation_xx_to_yy\", model=\"username/hf_tuto\")\ntranslator(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}